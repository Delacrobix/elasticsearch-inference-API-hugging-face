{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "C3EFzwK9YbaP",
   "metadata": {
    "id": "C3EFzwK9YbaP"
   },
   "source": [
    "# Elasticsearch Inference API & Hugging Face\n",
    "\n",
    "This notebook demonstrates how to use Hugging Face completions along with the Elasticsearch Inference API. This notebook is based on the article [Inference API & and Hugging Face](https://www.elastic.co/search-labs/blog/inference-api-and-hugging-face)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74bb30f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74bb30f2",
    "outputId": "185bf00e-67fb-4504-e56c-30f8c8872f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests elasticsearch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TgEea48gYnp5",
   "metadata": {
    "id": "TgEea48gYnp5"
   },
   "source": [
    "## Installing dependencies and importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b4668548",
   "metadata": {
    "id": "b4668548"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a30c9",
   "metadata": {},
   "source": [
    "## Setting up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab023",
   "metadata": {
    "id": "01aab023"
   },
   "outputs": [],
   "source": [
    "# os.environ[\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\"] = getpass(\n",
    "#     \"Enter your Hugging Face Inference Endpoint URL: \"\n",
    "# )\n",
    "# os.environ[\"ELASTICSEARCH_API_KEY\"] = getpass(\"Enter your Elasticsearch API key: \")\n",
    "# os.environ[\"ELASTICSEARCH_URL\"] = getpass(\"Enter your Elasticsearch URL: \")\n",
    "# os.environ[\"HUGGING_FACE_API_KEY\"] = getpass(\"Enter your Hugging Face API key: \")\n",
    "\n",
    "\n",
    "INDEX_NAME = \"company-blog-posts\"\n",
    "INFERENCE_ENDPOINT_ID = \"hugging-face-gpt-oss-safeguard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjbXck_gc9lY",
   "metadata": {
    "id": "tjbXck_gc9lY"
   },
   "source": [
    "## Elasticsearch Python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8cf922ab",
   "metadata": {
    "id": "8cf922ab"
   },
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(\n",
    "    os.environ[\"ELASTICSEARCH_URL\"], api_key=os.environ[\"ELASTICSEARCH_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832186f0",
   "metadata": {},
   "source": [
    "## Hugging Face completions inference endpoint setup\n",
    "\n",
    "Let’s create an Elasticsearch inference endpoint that uses the Hugging Face model. This endpoint will be used for chat completions, and this service will help us generate responses based on the dataset. To create this endpoint we’re using the [inference API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "40eba1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion inference endpoint created successfully: hugging-face-gpt-oss-safeguard\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resp = es_client.inference.put(\n",
    "        task_type=\"chat_completion\",\n",
    "        inference_id=INFERENCE_ENDPOINT_ID,\n",
    "        body={\n",
    "            \"service\": \"hugging_face\",\n",
    "            \"service_settings\": {\n",
    "                \"api_key\": os.environ[\"HUGGING_FACE_API_KEY\"],\n",
    "                \"url\": os.environ[\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\"],\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Chat completion inference endpoint created successfully:\", resp[\"inference_id\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error creating chat completion inference endpoint:\", {e})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iRX6_LuXdBy1",
   "metadata": {
    "id": "iRX6_LuXdBy1"
   },
   "source": [
    "## Index setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead65986",
   "metadata": {},
   "source": [
    "### Creating mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fc74243",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fc74243",
    "outputId": "d0f171e6-fcb4-48be-b9e6-11682410cf4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index company-blog-posts created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
    "                    \"copy_to\": \"semantic_field\",\n",
    "                },\n",
    "                \"author\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "                \"category\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "                \"content\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
    "                \"date\": {\"type\": \"date\"},\n",
    "                \"semantic_field\": {\"type\": \"semantic_text\"},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    es_client.indices.create(index=INDEX_NAME, body=mapping)\n",
    "    print(f\"Index {INDEX_NAME} created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sBDsH4VCZNBX",
   "metadata": {
    "id": "sBDsH4VCZNBX"
   },
   "source": [
    "### Ingesting data to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vuPAJRXLY_-J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuPAJRXLY_-J",
    "outputId": "29b17505-0150-4ea3-b4d0-cf5fd192d1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 documents indexed successfully\n"
     ]
    }
   ],
   "source": [
    "def build_data(json_file, index_name):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for doc in data:\n",
    "        yield {\"_index\": index_name, \"_source\": doc}\n",
    "\n",
    "\n",
    "try:\n",
    "    success, errors = helpers.bulk(es_client, build_data(\"dataset.json\", INDEX_NAME))\n",
    "    print(f\"{success} documents indexed successfully\")\n",
    "\n",
    "    if errors:\n",
    "        print(\"Errors during indexing:\", errors)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gZD2xqc-ZW88",
   "metadata": {
    "id": "gZD2xqc-ZW88"
   },
   "source": [
    "## Function to execute semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "R54-_-0IZITC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R54-_-0IZITC",
    "outputId": "a243c9d5-1687-449e-f12a-7d82a0f592b3"
   },
   "outputs": [],
   "source": [
    "def semantic_search(user_question: str, size: int = 5):\n",
    "    try:\n",
    "        response = es_client.search(\n",
    "            index=INDEX_NAME,\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"semantic\": {\n",
    "                        \"field\": \"semantic_field\",\n",
    "                        \"query\": user_question,\n",
    "                    }\n",
    "                },\n",
    "                \"size\": size,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"hits\": response[\"hits\"][\"hits\"],\n",
    "            \"total_hits\": len(response[\"hits\"][\"hits\"]),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching index: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a82dc6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits: 1\n",
      "{\n",
      "  \"id\": \"8\",\n",
      "  \"title\": \"Security update: encrypted data pipelines\",\n",
      "  \"author\": \"Daniel Vega\",\n",
      "  \"date\": \"2025-11-08\",\n",
      "  \"category\": \"security\",\n",
      "  \"content\": \"We have enhanced our data pipelines with full encryption in transit and at rest. This aligns with our company\\u2019s commitment to protecting customer data.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results = semantic_search(\n",
    "    user_question=\"Search for updates related to data encryption.\", size=1\n",
    ")\n",
    "\n",
    "print(f\"Total hits: {results['total_hits']}\")\n",
    "for hit in results[\"hits\"]:\n",
    "    print(f\"{json.dumps(hit['_source'], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ce435",
   "metadata": {},
   "source": [
    "## Generating completions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "530b687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat_completion(messages: list, inference_id: str = INFERENCE_ENDPOINT_ID):\n",
    "    try:\n",
    "\n",
    "        response = requests.post(\n",
    "            url=f\"{os.environ['ELASTICSEARCH_URL']}/_inference/chat_completion/{inference_id}/_stream\",\n",
    "            json={\n",
    "                \"messages\": messages,\n",
    "            },\n",
    "            headers={\n",
    "                \"Authorization\": f\"ApiKey {os.environ['ELASTICSEARCH_API_KEY']}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            stream=True,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        response.encoding = \"utf-8\"\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if line:\n",
    "                line = line.strip()\n",
    "\n",
    "                # Skip event lines like \"event: message\"\n",
    "                if line.startswith(\"event:\"):\n",
    "                    continue\n",
    "\n",
    "                # Process data lines\n",
    "                if line.startswith(\"data: \"):\n",
    "                    data_content = line[6:]  # Remove \"data: \" prefix\n",
    "\n",
    "                    if not data_content.strip() or data_content.strip() == \"[DONE]\":\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        chunk_data = json.loads(data_content)\n",
    "\n",
    "                        # Extract the content from the response structure\n",
    "                        if \"choices\" in chunk_data and len(chunk_data[\"choices\"]) > 0:\n",
    "                            choice = chunk_data[\"choices\"][0]\n",
    "                            if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
    "                                content = choice[\"delta\"][\"content\"]\n",
    "                                if content:\n",
    "                                    yield content\n",
    "\n",
    "                    except json.JSONDecodeError as json_err:\n",
    "                        print(f\"\\nJSON decode error: {json_err}\")\n",
    "                        print(f\"Problematic data: {data_content}\")\n",
    "                        continue\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        yield f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af051c",
   "metadata": {},
   "source": [
    "## RAG Chat with Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2e720494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderation_chat(\n",
    "    user_query: str,\n",
    "    inference_id: str = INFERENCE_ENDPOINT_ID,\n",
    "    ruleset_path: str = \"policies.txt\",\n",
    "):\n",
    "    print(\n",
    "        f\"Moderation chat invoked with QUERY: {user_query} and INFERENCE_ID: {inference_id}\"\n",
    "    )\n",
    "    search_results = semantic_search(user_query)\n",
    "    context_docs = search_results[\"hits\"]\n",
    "\n",
    "    try:\n",
    "        with open(ruleset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ruleset = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"ruleset.txt not found. Please provide the company policies file.\"\n",
    "        )\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a Safeguard moderation assistant.\n",
    "        Evaluate the provided article according to the company content policies.\n",
    "\n",
    "        The evaluation must follow these instructions:\n",
    "        - Only use the provided ruleset to judge compliance.\n",
    "        - Focus on tone, confidentiality, and adherence to internal communication standards.\n",
    "\n",
    "        Company ruleset:\n",
    "        {ruleset}\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, doc in enumerate(context_docs, 1):\n",
    "        title = doc[\"_source\"].get(\"title\", f\"Article {i}\")\n",
    "        content = doc[\"_source\"].get(\"content\", \"\")\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "            Evaluate this article:\n",
    "            Title: {title}\n",
    "            Content body:\n",
    "            {content}\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        print(\"\\n------------------------------------------------\")\n",
    "        print(\n",
    "            f\"Evaluating article {i}/{len(context_docs)}\\n TITLE: {title} \\n CONTENT: {content}\\n\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            full_response = \"\"\n",
    "            for chunk in stream_chat_completion(messages, inference_id=inference_id):\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "                full_response += chunk\n",
    "\n",
    "            results.append({\"title\": title, \"response\": full_response.strip()})\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Error evaluating {title}: {e}\")\n",
    "            results.append({\"title\": title, \"response\": None, \"error\": str(e)})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e04a7795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moderation chat invoked with QUERY: Find posts explaining debugging or authentication issues and INFERENCE_ID: hugging-face-gpt-oss-safeguard\n",
      "\n",
      "------------------------------------------------\n",
      "Evaluating article 1/5\n",
      " TITLE: Debugging authentication issues \n",
      " CONTENT: When debugging authentication, never share production tokens or client credentials. Always use our test environment and anonymized data for public examples.\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "Evaluating article 1/5\n",
      " TITLE: Debugging authentication issues \n",
      " CONTENT: When debugging authentication, never share production tokens or client credentials. Always use our test environment and anonymized data for public examples.\n",
      "\n",
      "**Evaluation Result: PASS**  \n",
      "The article does **not** contain any of the prohibited content categories:\n",
      "\n",
      "**Evaluation Result: PASS**  \n",
      "The article does **not** contain any of the prohibited content categories:\n",
      "\n",
      "1. No confidential project names, pricing, or sensitive credentials are disclosed.  \n",
      "2. The tone is factual and professional1. No confidential project names, pricing, or sensitive credentials are disclosed.  \n",
      "2. The tone is factual and professional.  \n",
      "3. There are no comparative or mocking statements toward competitors.  \n",
      "4. Personal or client information is not.  \n",
      "3. There are no comparative or mocking statements toward competitors.  \n",
      "4. Personal or client information is not included.  \n",
      "5. Instead of encouraging unsafe practices, it advises against sharing production tokens and promotes the use of test environments included.  \n",
      "5. Instead of encouraging unsafe practices, it advises against sharing production tokens and promotes the use of test environments and anonymized data.\n",
      "\n",
      "**Conclusion:** The content is compliant and falls under the safe/content category. and anonymized data.\n",
      "\n",
      "**Conclusion:** The content is compliant and falls under the safe/content category.\n",
      "------------------------------------------------\n",
      "Evaluating article 2/5\n",
      " TITLE: Meet our competitors head-on! \n",
      " CONTENT: Unlike BrandX, our software is actually stable and bug-free. This article compares us directly to competitors and mocks their quality issues.\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "Evaluating article 2/5\n",
      " TITLE: Meet our competitors head-on! \n",
      " CONTENT: Unlike BrandX, our software is actually stable and bug-free. This article compares us directly to competitors and mocks their quality issues.\n",
      "\n",
      "**Compliance Evaluation**\n",
      "\n",
      "**Article Summary**  \n",
      "- Title: “Meet our competitors head**Compliance Evaluation**\n",
      "\n",
      "**Article Summary**  \n",
      "- Title: “Meet our competitors head‑on!”  \n",
      "- Content: “Unlike BrandX, our software is actually stable and bug‑free. This article‑on!”  \n",
      "- Content: “Unlike BrandX, our software is actually stable and bug‑free. This article compares us directly to competitors and mocks their quality issues.”\n",
      "\n",
      "**Rule Violations**\n",
      "\n",
      "| # | Vi compares us directly to competitors and mocks their quality issues.”\n",
      "\n",
      "**Rule Violations**\n",
      "\n",
      "| # | Violation Category | Explanation |\n",
      "|---|--------------------|-------------|\n",
      "| 3 | Unprofessional/Mocking tone towardolation Category | Explanation |\n",
      "|---|--------------------|-------------|\n",
      "| 3 | Unprofessional/Mocking tone toward competitors | The post explicitly states negative observations about “BrandX” and claims to mock its quality issues. This is competitors | The post explicitly states negative observations about “BrandX” and claims to mock its quality issues. This is a direct comparative attack that falls under the policy’s prohibition of unprofessional or negative remarks toward competitors. |\n",
      "| 4 a direct comparative attack that falls under the policy’s prohibition of unprofessional or negative remarks toward competitors. |\n",
      "| 4 | Comparative claim (in a negative tone) | The comparison is not presented in a neutral or fact‑based manner | Comparative claim (in a negative tone) | The comparison is not presented in a neutral or fact‑based manner; it is presented as a taunt, violating the policy against negative comparative statements. |\n",
      "\n",
      "**Conclusion**  \n",
      "The article; it is presented as a taunt, violating the policy against negative comparative statements. |\n",
      "\n",
      "**Conclusion**  \n",
      "The article violates the company’s content policies, specifically the rule against unprofessional or mocking language toward competitors. It should be revised or removed prior to publication. violates the company’s content policies, specifically the rule against unprofessional or mocking language toward competitors. It should be revised or removed prior to publication.\n",
      "------------------------------------------------\n",
      "Evaluating article 3/5\n",
      " TITLE: Security update: encrypted data pipelines \n",
      " CONTENT: We have enhanced our data pipelines with full encryption in transit and at rest. This aligns with our company’s commitment to protecting customer data.\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "Evaluating article 3/5\n",
      " TITLE: Security update: encrypted data pipelines \n",
      " CONTENT: We have enhanced our data pipelines with full encryption in transit and at rest. This aligns with our company’s commitment to protecting customer data.\n",
      "\n",
      "**Compliance Evaluation**\n",
      "\n",
      "The article:\n",
      "\n",
      "*Title:* \"Security update: encrypted data pipelines**Compliance Evaluation**\n",
      "\n",
      "The article:\n",
      "\n",
      "*Title:* \"Security update: encrypted data pipelines\"  \n",
      "*Body:* “We have enhanced our data pipelines with full encryption in transit and at rest. This aligns\"  \n",
      "*Body:* “We have enhanced our data pipelines with full encryption in transit and at rest. This aligns with our company’s commitment to protecting customer data.”\n",
      "\n",
      "**Analysis**\n",
      "\n",
      "- **Confidential Information**: No internal project c with our company’s commitment to protecting customer data.”\n",
      "\n",
      "**Analysis**\n",
      "\n",
      "- **Confidential Information**: No internal project codenames, API keys, or proprietary details are disclosed.\n",
      "- **Pricing/Financial Details**: None provided.\n",
      "odenames, API keys, or proprietary details are disclosed.\n",
      "- **Pricing/Financial Details**: None provided.\n",
      "- **Tone**: The language is professional, factual, and respectful.\n",
      "- **Personal/Client Data**: No- **Tone**: The language is professional, factual, and respectful.\n",
      "- **Personal/Client Data**: No personal identifiers or client specifics are mentioned.\n",
      "- **Safety**: No encouragement of unsafe practices or sharing personal identifiers or client specifics are mentioned.\n",
      "- **Safety**: No encouragement of unsafe practices or sharing of credentials.\n",
      "\n",
      "**Verdict**: **Safe content** – satisfies all guidelines. No rule violations detected. of credentials.\n",
      "\n",
      "**Verdict**: **Safe content** – satisfies all guidelines. No rule violations detected.\n",
      "------------------------------------------------\n",
      "Evaluating article 4/5\n",
      " TITLE: Client feedback and internal complaints \n",
      " CONTENT: Some clients are hard to deal with and frankly don’t know how to use our software. In this post I’ll share examples from their support tickets.\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "Evaluating article 4/5\n",
      " TITLE: Client feedback and internal complaints \n",
      " CONTENT: Some clients are hard to deal with and frankly don’t know how to use our software. In this post I’ll share examples from their support tickets.\n",
      "\n",
      "**Evaluation Result: Violates**Evaluation Result: Violates Rule 3 – Unprofessional or Mocking Language Toward Clients**\n",
      "\n",
      "The article contains a tone that is disrespectful toward Rule 3 – Unprofessional or Mocking Language Toward Clients**\n",
      "\n",
      "The article contains a tone that is disrespectful toward clients, specifically stating that “Some clients are hard to deal with and frankly don’t know how to use our software clients, specifically stating that “Some clients are hard to deal with and frankly don’t know how to use our software.” This language is offensive, unprofessional, and unsupported by factual context, thereby contravening the company’s internal communication.” This language is offensive, unprofessional, and unsupported by factual context, thereby contravening the company’s internal communication standards regarding respectful client engagement. No other rules (confidential data, pricing, personal data, or unsafe practices) are invoked by the content provided. standards regarding respectful client engagement. No other rules (confidential data, pricing, personal data, or unsafe practices) are invoked by the content provided.\n",
      "------------------------------------------------\n",
      "Evaluating article 5/5\n",
      " TITLE: Casual Friday: fun at the office! \n",
      " CONTENT: Sharing a few photos of our last Casual Friday. Everyone had a great time! This blog post respects internal media policy, with consent from all participants.\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "Evaluating article 5/5\n",
      " TITLE: Casual Friday: fun at the office! \n",
      " CONTENT: Sharing a few photos of our last Casual Friday. Everyone had a great time! This blog post respects internal media policy, with consent from all participants.\n",
      "\n",
      "**Evaluation Result:** SAFE\n",
      "\n",
      "**Evaluation Result:** SAFE\n",
      "\n",
      "**Reasoning:**\n",
      "- The article contains no internal project names, pricing details, or any other confidential information.**Reasoning:**\n",
      "- The article contains no internal project names, pricing details, or any other confidential information.  \n",
      "- The tone is informal but respectful, with no negative or mocking language toward competitors or clients.  \n",
      "- Personal data  \n",
      "- The tone is informal but respectful, with no negative or mocking language toward competitors or clients.  \n",
      "- Personal data is handled appropriately by stating that consent was obtained from all participants and the post follows internal media policy.  \n",
      "- No is handled appropriately by stating that consent was obtained from all participants and the post follows internal media policy.  \n",
      "- No unsafe practices or sensitive credentials are shared.  \n",
      "\n",
      "Hence, the article complies with the company content policies. unsafe practices or sensitive credentials are shared.  \n",
      "\n",
      "Hence, the article complies with the company content policies."
     ]
    }
   ],
   "source": [
    "response = moderation_chat(\n",
    "    user_query=\"Find posts explaining debugging or authentication issues\"\n",
    ")\n",
    "# Find posts explaining debugging or authentication issues.\n",
    "\n",
    "# “Find articles about analytics dashboards or visualization tools.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion inference endpoint created successfully: gpt-oss-20b-endpoint\n"
     ]
    }
   ],
   "source": [
    "NEW_INFERENCE_ENDPOINT_ID = \"gpt-oss-20b-endpoint\"\n",
    "NEW_INFERENCE_ENDPOINT_URL = (\n",
    "    \"https://aajflm.us-east-2.aws.endpoints.huggingface.cloud/v1/chat/completions\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = es_client.inference.put(\n",
    "        task_type=\"chat_completion\",\n",
    "        inference_id=NEW_INFERENCE_ENDPOINT_ID,\n",
    "        body={\n",
    "            \"service\": \"hugging_face\",\n",
    "            \"service_settings\": {\n",
    "                \"api_key\": os.environ[\"HUGGING_FACE_API_KEY\"],\n",
    "                \"url\": NEW_INFERENCE_ENDPOINT_URL,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Chat completion inference endpoint created successfully:\", resp[\"inference_id\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error creating chat completion inference endpoint:\", {e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1cfa3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moderation chat invoked with QUERY: Find posts explaining debugging or authentication issues and INFERENCE_ID: gpt-oss-20b-endpoint\n",
      "\n",
      "------------------------------------------------\n",
      "Evaluating article 1/5\n",
      " TITLE: Debugging authentication issues \n",
      " CONTENT: When debugging authentication, never share production tokens or client credentials. Always use our test environment and anonymized data for public examples.\n",
      "\n",
      "**Compliance Evaluation**\n",
      "\n",
      "- **Rule 1 – Confidential Information:** No internal project names, client data, or non‑public credentials are disclosed.  \n",
      "- **Rule 2 – Internal Pricing:** No pricing or financial details are shared.  \n",
      "- **Rule 3 – Unprofessional Language:** The tone is factual and respectful; no mocking or negative comparisons.  \n",
      "- **Rule 4 – Personal Data:** No identifiable client or employee information appears.  \n",
      "- **Rule 5 – Unsafe Practices:** The post explicitly advises against sharing production tokens or credentials, discouraging unsafe behavior.  \n",
      "\n",
      "**Result:** The article complies with all company content policies. No violations are detected.\n",
      "------------------------------------------------\n",
      "Evaluating article 2/5\n",
      " TITLE: Meet our competitors head-on! \n",
      " CONTENT: Unlike BrandX, our software is actually stable and bug-free. This article compares us directly to competitors and mocks their quality issues.\n",
      "\n",
      "**Violation identified**\n",
      "\n",
      "**Rule violated:**  \n",
      "**3. Use of unprofessional, offensive, or mocking language toward clients or competitors.**\n",
      "\n",
      "**Explanation:**  \n",
      "The article directly compares the company to its competitor (BrandX) in a mocking tone, stating “Unlike BrandX, our software is actually stable and bug‑free.” This violates the company's internal communication standard that requires a professional, respectful, and factual tone, especially when referencing competitors.\n",
      "------------------------------------------------\n",
      "Evaluating article 3/5\n",
      " TITLE: Security update: encrypted data pipelines \n",
      " CONTENT: We have enhanced our data pipelines with full encryption in transit and at rest. This aligns with our company’s commitment to protecting customer data.\n",
      "\n",
      "**Evaluation**  \n",
      "- No confidential project names, internal pricing, or credentials are disclosed.  \n",
      "- The tone is professional, factual, and respectful.  \n",
      "- There are no references to competitors or negative remarks.  \n",
      "- No personal or client data is shared.  \n",
      "- No unsafe practices are encouraged.\n",
      "\n",
      "**Verdict**: The article complies with all company content policies and is considered **Safe Content**.\n",
      "------------------------------------------------\n",
      "Evaluating article 4/5\n",
      " TITLE: Client feedback and internal complaints \n",
      " CONTENT: Some clients are hard to deal with and frankly don’t know how to use our software. In this post I’ll share examples from their support tickets.\n",
      "\n",
      "**Evaluation**  \n",
      "\n",
      "The provided article violates the company content policy.\n",
      "\n",
      "| Rule Violated | Reason |\n",
      "|---------------|--------|\n",
      "| **3. Use of unprofessional, offensive, or mocking language toward clients or competitors** | The tone is dismissive and disparaging (“Some clients are hard to deal with and frankly don’t know how to use our software.”), which is unprofessional and disrespectful to client stakeholders. |\n",
      "| **4. Inclusion of personal or client information without explicit consent** | The statement that the author will “share examples from their support tickets” suggests disclosing specific client interactions, which may contain personal or identifiable data. Sharing such information without consent breaches the policy. |\n",
      "\n",
      "**Conclusion**: The article does not meet the acceptable standards for corporate communication. It should be revised to maintain a professional, respectful tone and to avoid revealing any private client details.\n",
      "------------------------------------------------\n",
      "Evaluating article 5/5\n",
      " TITLE: Casual Friday: fun at the office! \n",
      " CONTENT: Sharing a few photos of our last Casual Friday. Everyone had a great time! This blog post respects internal media policy, with consent from all participants.\n",
      "\n",
      "**Evaluation Result: YES – The article is compliant with the company content policy.**\n",
      "\n",
      "**Why?**\n",
      "\n",
      "| Criteria | Assessment |\n",
      "|----------|------------|\n",
      "| 1. Confidential information | No internal project names, client data, or proprietary details are mentioned. |\n",
      "| 2. Internal pricing/financial data | No pricing or financial figures are provided. |\n",
      "| 3. Unprofessional/offensive language | The tone is friendly but respectful; there is no mocking or negative language toward clients or competitors. |\n",
      "| 4. Personal data | Participants are mentioned only in aggregate, and the article states that all have provided consent. |\n",
      "| 5. Encourages unsafe practices | Not applicable; no tokens, passwords, or other credentials are shared. |\n",
      "\n",
      "**Conclusion:**  \n",
      "The post describes a routine company event, uses general language, and explicitly notes consent from participants. It falls entirely within the \"Safe Content\" categories and meets the company’s internal communication standards. No action is needed."
     ]
    }
   ],
   "source": [
    "response = moderation_chat(\n",
    "    \"Find posts explaining debugging or authentication issues\",\n",
    "    inference_id=NEW_INFERENCE_ENDPOINT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C2WKnj8UZa7g",
   "metadata": {
    "id": "C2WKnj8UZa7g"
   },
   "source": [
    "## Deleting\n",
    "\n",
    "Delete the resources used to prevent them from consuming resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TgqFHEhPZfAd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgqFHEhPZfAd",
    "outputId": "675e161b-f7e3-43f8-d3cc-f9e7fa72f6aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup - Delete Index\n",
    "es_client.indices.delete(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5df6836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'pipelines': [], 'indexes': []})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup - Delete Inference Endpoints\n",
    "es_client.inference.delete(inference_id=INFERENCE_ENDPOINT_ID)\n",
    "\n",
    "# es_client.inference.delete(inference_id=NEW_INFERENCE_ENDPOINT_ID)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

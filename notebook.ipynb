{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "C3EFzwK9YbaP",
   "metadata": {
    "id": "C3EFzwK9YbaP"
   },
   "source": [
    "# Safety models with Elasticsearch Inference API & Hugging Face\n",
    "\n",
    "This notebook demonstrates how to use Hugging Face completions along with the Elasticsearch Inference API. This notebook is based on the article [Safety models with Elasticsearch Inference API & Hugging Face](https://www.elastic.co/search-labs/blog/safety-models-inference-api-and-hugging-face)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb30f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74bb30f2",
    "outputId": "185bf00e-67fb-4504-e56c-30f8c8872f83"
   },
   "outputs": [],
   "source": [
    "%pip install requests elasticsearch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TgEea48gYnp5",
   "metadata": {
    "id": "TgEea48gYnp5"
   },
   "source": [
    "## Installing dependencies and importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4668548",
   "metadata": {
    "id": "b4668548"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a30c9",
   "metadata": {},
   "source": [
    "## Setting up environment variables\n",
    "\n",
    "Configure API keys and URLs for Elasticsearch and Hugging Face, along with index name and inference endpoint identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab023",
   "metadata": {
    "id": "01aab023"
   },
   "outputs": [],
   "source": [
    "os.environ[\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\"] = getpass(\n",
    "    \"Enter your Hugging Face Inference Endpoint URL: \"\n",
    ")\n",
    "os.environ[\"ELASTICSEARCH_API_KEY\"] = getpass(\"Enter your Elasticsearch API key: \")\n",
    "os.environ[\"ELASTICSEARCH_URL\"] = getpass(\"Enter your Elasticsearch URL: \")\n",
    "os.environ[\"HUGGING_FACE_API_KEY\"] = getpass(\"Enter your Hugging Face API key: \")\n",
    "\n",
    "\n",
    "INDEX_NAME = \"company-blog-posts\"\n",
    "INFERENCE_ENDPOINT_ID = \"hugging-face-gpt-oss-safeguard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjbXck_gc9lY",
   "metadata": {
    "id": "tjbXck_gc9lY"
   },
   "source": [
    "## Elasticsearch Python client\n",
    "\n",
    "Initialize the Elasticsearch client using the configured URL and API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf922ab",
   "metadata": {
    "id": "8cf922ab"
   },
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(\n",
    "    os.environ[\"ELASTICSEARCH_URL\"], api_key=os.environ[\"ELASTICSEARCH_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832186f0",
   "metadata": {},
   "source": [
    "## Hugging Face completions inference endpoint setup\n",
    "\n",
    "Create an Elasticsearch inference endpoint that connects to the Hugging Face model for generating responses based on blog posts and policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eba1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = es_client.inference.put(\n",
    "        task_type=\"chat_completion\",\n",
    "        inference_id=INFERENCE_ENDPOINT_ID,\n",
    "        body={\n",
    "            \"service\": \"hugging_face\",\n",
    "            \"service_settings\": {\n",
    "                \"api_key\": os.environ[\"HUGGING_FACE_API_KEY\"],\n",
    "                \"url\": os.environ[\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\"],\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Chat completion inference endpoint created successfully:\", resp[\"inference_id\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error creating chat completion inference endpoint:\", {e})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iRX6_LuXdBy1",
   "metadata": {
    "id": "iRX6_LuXdBy1"
   },
   "source": [
    "## Index setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead65986",
   "metadata": {},
   "source": [
    "### Creating mappings\n",
    "\n",
    "Define field types and properties including `semantic_text` with ELSER model for embeddings and `copy_to` properties for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc74243",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fc74243",
    "outputId": "d0f171e6-fcb4-48be-b9e6-11682410cf4b"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
    "                    \"copy_to\": \"semantic_field\",\n",
    "                },\n",
    "                \"author\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "                \"category\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "                \"content\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
    "                \"date\": {\"type\": \"date\"},\n",
    "                \"semantic_field\": {\"type\": \"semantic_text\"},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    es_client.indices.create(index=INDEX_NAME, body=mapping)\n",
    "    print(f\"Index {INDEX_NAME} created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sBDsH4VCZNBX",
   "metadata": {
    "id": "sBDsH4VCZNBX"
   },
   "source": [
    "### Ingesting data to Elasticsearch\n",
    "\n",
    "Use the bulk API to ingest the blog posts dataset into Elasticsearch from a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vuPAJRXLY_-J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuPAJRXLY_-J",
    "outputId": "29b17505-0150-4ea3-b4d0-cf5fd192d1b6"
   },
   "outputs": [],
   "source": [
    "def build_data(json_file, index_name):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for doc in data:\n",
    "        yield {\"_index\": index_name, \"_source\": doc}\n",
    "\n",
    "\n",
    "try:\n",
    "    success, errors = helpers.bulk(es_client, build_data(\"dataset.json\", INDEX_NAME))\n",
    "    print(f\"{success} documents indexed successfully\")\n",
    "\n",
    "    if errors:\n",
    "        print(\"Errors during indexing:\", errors)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gZD2xqc-ZW88",
   "metadata": {
    "id": "gZD2xqc-ZW88"
   },
   "source": [
    "## Function to execute semantic search\n",
    "\n",
    "Query the `semantic_field` to retrieve relevant articles based on user questions, leveraging ELSER-generated embeddings for semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R54-_-0IZITC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R54-_-0IZITC",
    "outputId": "a243c9d5-1687-449e-f12a-7d82a0f592b3"
   },
   "outputs": [],
   "source": [
    "def semantic_search(user_question: str, size: int = 5):\n",
    "    try:\n",
    "        response = es_client.search(\n",
    "            index=INDEX_NAME,\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"semantic\": {\n",
    "                        \"field\": \"semantic_field\",\n",
    "                        \"query\": user_question,\n",
    "                    }\n",
    "                },\n",
    "                \"size\": size,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"hits\": response[\"hits\"][\"hits\"],\n",
    "            \"total_hits\": len(response[\"hits\"][\"hits\"]),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching index: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24d3b0",
   "metadata": {},
   "source": [
    "### Testing semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82dc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = semantic_search(\n",
    "    user_question=\"Search for updates related to data encryption.\", size=1\n",
    ")\n",
    "\n",
    "print(f\"Total hits: {results['total_hits']}\")\n",
    "for hit in results[\"hits\"]:\n",
    "    print(f\"{json.dumps(hit['_source'], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ce435",
   "metadata": {},
   "source": [
    "## Generating completions function\n",
    "\n",
    "Send messages to the Elasticsearch inference endpoint with streaming support, processing server-sent events to extract model responses in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat_completion(messages: list, inference_id: str = INFERENCE_ENDPOINT_ID):\n",
    "    try:\n",
    "\n",
    "        response = requests.post(\n",
    "            url=f\"{os.environ['ELASTICSEARCH_URL']}/_inference/chat_completion/{inference_id}/_stream\",\n",
    "            json={\n",
    "                \"messages\": messages,\n",
    "            },\n",
    "            headers={\n",
    "                \"Authorization\": f\"ApiKey {os.environ['ELASTICSEARCH_API_KEY']}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            stream=True,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        response.encoding = \"utf-8\"\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if line:\n",
    "                line = line.strip()\n",
    "\n",
    "                # Skip event lines like \"event: message\"\n",
    "                if line.startswith(\"event:\"):\n",
    "                    continue\n",
    "\n",
    "                # Process data lines\n",
    "                if line.startswith(\"data: \"):\n",
    "                    data_content = line[6:]  # Remove \"data: \" prefix\n",
    "\n",
    "                    if not data_content.strip() or data_content.strip() == \"[DONE]\":\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        chunk_data = json.loads(data_content)\n",
    "\n",
    "                        # Extract the content from the response structure\n",
    "                        if \"choices\" in chunk_data and len(chunk_data[\"choices\"]) > 0:\n",
    "                            choice = chunk_data[\"choices\"][0]\n",
    "                            if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
    "                                content = choice[\"delta\"][\"content\"]\n",
    "                                if content:\n",
    "                                    yield content\n",
    "\n",
    "                    except json.JSONDecodeError as json_err:\n",
    "                        print(f\"\\nJSON decode error: {json_err}\")\n",
    "                        print(f\"Problematic data: {data_content}\")\n",
    "                        continue\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        yield f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af051c",
   "metadata": {},
   "source": [
    "## RAG Chat with Streaming\n",
    "\n",
    "Combine semantic search and chat completions to evaluate blog posts using gpt-oss-safeguard-20b model, providing detailed rule-based assessments with explanations for policy violations and compliance status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e720494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderation_chat(\n",
    "    user_query: str,\n",
    "    inference_id: str = INFERENCE_ENDPOINT_ID,\n",
    "    ruleset_path: str = \"policies.txt\",\n",
    "):\n",
    "    print(\n",
    "        f\"Moderation chat invoked with QUERY: {user_query} and INFERENCE_ID: {inference_id}\"\n",
    "    )\n",
    "    search_results = semantic_search(user_query)\n",
    "    context_docs = search_results[\"hits\"]\n",
    "\n",
    "    print(\"docs: \", context_docs)\n",
    "\n",
    "    try:\n",
    "        with open(ruleset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ruleset = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"file not found. Please provide the company policies file.\"\n",
    "        )\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a Safeguard moderation assistant.\n",
    "        Evaluate the provided article according to the company content policies.\n",
    "\n",
    "        The evaluation must follow these instructions:\n",
    "        - Only use the provided ruleset to judge compliance.\n",
    "        - Focus on tone, confidentiality, and adherence to internal communication standards.\n",
    "\n",
    "        Company ruleset:\n",
    "        {ruleset}\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, doc in enumerate(context_docs, 1):\n",
    "        title = doc[\"_source\"].get(\"title\", f\"Article {i}\")\n",
    "        content = doc[\"_source\"].get(\"content\", \"\")\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "            Evaluate this article:\n",
    "            Title: {title}\n",
    "            Content body:\n",
    "            {content}\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        print(\"\\n------------------------------------------------\")\n",
    "        print(\n",
    "            f\"Evaluating article {i}/{len(context_docs)}\\n TITLE: {title} \\n CONTENT: {content}\\n\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            full_response = \"\"\n",
    "            for chunk in stream_chat_completion(messages, inference_id=inference_id):\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "                full_response += chunk\n",
    "\n",
    "            results.append({\"title\": title, \"response\": full_response.strip()})\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Error evaluating {title}: {e}\")\n",
    "            results.append({\"title\": title, \"response\": None, \"error\": str(e)})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = moderation_chat(\n",
    "    user_query=\"Find posts explaining debugging or authentication issues\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4bb47",
   "metadata": {},
   "source": [
    "## Inference endpoint for no-safety model\n",
    "\n",
    "Create a second inference endpoint using gpt-oss-20b (general-purpose model) for comparison with the safeguard version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_INFERENCE_ENDPOINT_ID = \"gpt-oss-20b-endpoint\"\n",
    "NEW_INFERENCE_ENDPOINT_URL = (\n",
    "    \"https://aajflm.us-east-2.aws.endpoints.huggingface.cloud/v1/chat/completions\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = es_client.inference.put(\n",
    "        task_type=\"chat_completion\",\n",
    "        inference_id=NEW_INFERENCE_ENDPOINT_ID,\n",
    "        body={\n",
    "            \"service\": \"hugging_face\",\n",
    "            \"service_settings\": {\n",
    "                \"api_key\": os.environ[\"HUGGING_FACE_API_KEY\"],\n",
    "                \"url\": NEW_INFERENCE_ENDPOINT_URL,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Chat completion inference endpoint created successfully:\", resp[\"inference_id\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error creating chat completion inference endpoint:\", {e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfa3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = moderation_chat(\n",
    "    \"Find posts explaining debugging or authentication issues\",\n",
    "    inference_id=NEW_INFERENCE_ENDPOINT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C2WKnj8UZa7g",
   "metadata": {
    "id": "C2WKnj8UZa7g"
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the index and inference endpoints to prevent consuming resources after completing the evaluation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TgqFHEhPZfAd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgqFHEhPZfAd",
    "outputId": "675e161b-f7e3-43f8-d3cc-f9e7fa72f6aa"
   },
   "outputs": [],
   "source": [
    "# Cleanup - Delete Index\n",
    "es_client.indices.delete(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - Delete Inference Endpoints\n",
    "es_client.inference.delete(inference_id=INFERENCE_ENDPOINT_ID)\n",
    "\n",
    "es_client.inference.delete(inference_id=NEW_INFERENCE_ENDPOINT_ID)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

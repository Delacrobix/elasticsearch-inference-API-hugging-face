{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "C3EFzwK9YbaP",
      "metadata": {
        "id": "C3EFzwK9YbaP"
      },
      "source": [
        "# Using Elasticsearch Inference API along Hugging Face models\n",
        "\n",
        "This notebook demonstrates how to use the Elasticsearch Inference API along with Hugging Face models to build a question and answer system. This notebook is based on the [Using Elasticsearch Inference API along Hugging Face models](https://www.elastic.co/search-labs/blog/elasticsearch-inference-api-and-hugging-face)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "74bb30f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74bb30f2",
        "outputId": "185bf00e-67fb-4504-e56c-30f8c8872f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install requests elasticsearch -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TgEea48gYnp5",
      "metadata": {
        "id": "TgEea48gYnp5"
      },
      "source": [
        "## Installing dependencies and importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b4668548",
      "metadata": {
        "id": "b4668548"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import textwrap\n",
        "import re\n",
        "import time\n",
        "\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7a30c9",
      "metadata": {},
      "source": [
        "## Setting up environment variables\n",
        "\n",
        "Configure API keys and URLs for Elasticsearch and Hugging Face, along with the index name and inference endpoint identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "01aab023",
      "metadata": {
        "id": "01aab023"
      },
      "outputs": [],
      "source": [
        "ELASTICSEARCH_API_KEY = os.getenv(\"ELASTICSEARCH_API_KEY\")\n",
        "ELASTICSEARCH_URL = os.getenv(\"ELASTICSEARCH_URL\")\n",
        "HUGGING_FACE_API_KEY = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
        "HUGGING_FACE_INFERENCE_ENDPOINT_URL = os.getenv(\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\")\n",
        "\n",
        "\n",
        "INDEX_NAME = \"blog-posts\"\n",
        "INFERENCE_ENDPOINT_ID = \"hugging-face-smollm3-3b\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tjbXck_gc9lY",
      "metadata": {
        "id": "tjbXck_gc9lY"
      },
      "source": [
        "## Elasticsearch Python client\n",
        "\n",
        "Initialize the Elasticsearch client using the configured URL and API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8cf922ab",
      "metadata": {
        "id": "8cf922ab"
      },
      "outputs": [],
      "source": [
        "es_client = Elasticsearch(ELASTICSEARCH_URL, api_key=ELASTICSEARCH_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832186f0",
      "metadata": {},
      "source": [
        "## Hugging Face completions inference endpoint setup\n",
        "\n",
        "Create an Elasticsearch inference endpoint that connects to the Hugging Face model for generating responses based on blog articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "40eba1a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat completion inference endpoint created successfully: hugging-face-smollm3-3b\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    resp = es_client.inference.put(\n",
        "        task_type=\"chat_completion\",\n",
        "        inference_id=INFERENCE_ENDPOINT_ID,\n",
        "        body={\n",
        "            \"service\": \"hugging_face\",\n",
        "            \"service_settings\": {\n",
        "                \"api_key\": HUGGING_FACE_API_KEY,\n",
        "                \"url\": HUGGING_FACE_INFERENCE_ENDPOINT_URL,\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Chat completion inference endpoint created successfully:\",\n",
        "        resp[\"inference_id\"],\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Error creating chat completion inference endpoint:\", {e})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21438e7e",
      "metadata": {},
      "source": [
        "### Creating index mapping\n",
        "\n",
        "Define field types and properties for the blog articles index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2fbe4d1c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error creating index: BadRequestError(400, 'resource_already_exists_exception', 'index [blog-posts/DRWJYsymSwuHPcvOFSM_DA] already exists')\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    mapping = {\n",
        "        \"mappings\": {\n",
        "            \"properties\": {\n",
        "                \"id\": {\"type\": \"keyword\"},\n",
        "                \"title\": {\n",
        "                    \"type\": \"text\",\n",
        "                    \"copy_to\": \"semantic_field\",\n",
        "                    \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
        "                },\n",
        "                \"author\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
        "                \"category\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
        "                \"content\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
        "                \"date\": {\"type\": \"date\"},\n",
        "                \"semantic_field\": {\"type\": \"semantic_text\"},\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    es_client.indices.create(index=INDEX_NAME, body=mapping)\n",
        "    print(f\"Index {INDEX_NAME} created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating index: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0dd56355",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15 documents indexed successfully\n"
          ]
        }
      ],
      "source": [
        "def build_data(json_file, index_name):\n",
        "    with open(json_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for doc in data:\n",
        "        action = {\"_index\": index_name, \"_source\": doc}\n",
        "        yield action\n",
        "\n",
        "\n",
        "try:\n",
        "    success, failed = helpers.bulk(\n",
        "        es_client,\n",
        "        build_data(\"dataset.json\", INDEX_NAME),\n",
        "    )\n",
        "    print(f\"{success} documents indexed successfully\")\n",
        "\n",
        "    if failed:\n",
        "        print(f\"Errors: {failed}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afce36a4",
      "metadata": {},
      "source": [
        "## Semantic search function\n",
        "\n",
        "Function to search for relevant articles using Elasticsearch semantic search capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e99600e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_semantic_search(query_text, index_name=INDEX_NAME, size=5):\n",
        "    try:\n",
        "        query = {\n",
        "            \"query\": {\n",
        "                \"semantic\": {\n",
        "                    \"field\": \"semantic_field\",\n",
        "                    \"query\": query_text,\n",
        "                }\n",
        "            },\n",
        "            \"size\": size,\n",
        "        }\n",
        "\n",
        "        response = es_client.search(index=index_name, body=query)\n",
        "        hits = response[\"hits\"][\"hits\"]\n",
        "\n",
        "        return hits\n",
        "    except Exception as e:\n",
        "        print(f\"Semantic search error: {str(e)}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "530b687e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_chat_completion(messages: list, inference_id: str = INFERENCE_ENDPOINT_ID):\n",
        "    url = f\"{ELASTICSEARCH_URL}/_inference/chat_completion/{inference_id}/_stream\"\n",
        "    payload = {\"messages\": messages}\n",
        "    headers = {\n",
        "        \"Authorization\": f\"ApiKey {ELASTICSEARCH_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, headers=headers, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        for line in response.iter_lines(decode_unicode=True):\n",
        "            if line:\n",
        "                line = line.strip()\n",
        "\n",
        "                if line.startswith(\"event:\"):\n",
        "                    continue\n",
        "\n",
        "                if line.startswith(\"data: \"):\n",
        "                    data_content = line[6:]\n",
        "\n",
        "                    if not data_content.strip() or data_content.strip() == \"[DONE]\":\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        chunk_data = json.loads(data_content)\n",
        "\n",
        "                        if \"choices\" in chunk_data and len(chunk_data[\"choices\"]) > 0:\n",
        "                            choice = chunk_data[\"choices\"][0]\n",
        "                            if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
        "                                content = choice[\"delta\"][\"content\"]\n",
        "                                if content:\n",
        "                                    yield content\n",
        "\n",
        "                    except json.JSONDecodeError as json_err:\n",
        "                        print(f\"\\nJSON decode error: {json_err}\")\n",
        "                        print(f\"Problematic data: {data_content}\")\n",
        "                        continue\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        yield f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc966da8",
      "metadata": {},
      "source": [
        "## Article recommendation system with latency tracking\n",
        "\n",
        "SmolLM3-3B claims to deliver latencies between 40-200ms. We'll build a recommendation system that tracks latency at each stage to validate these claims while generating personalized article recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "2b2efa3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_articles(search_query, index_name=INDEX_NAME, max_articles=5):\n",
        "    latencies = {}\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ” Search Query: {search_query}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    articles = perform_semantic_search(search_query, index_name, size=max_articles)\n",
        "\n",
        "    if not articles:\n",
        "        print(\"âŒ No relevant articles found.\")\n",
        "        return None, None, latencies\n",
        "\n",
        "    print(f\"âœ… Found {len(articles)} relevant articles\\n\")\n",
        "\n",
        "    # Build context with found articles\n",
        "    context = \"Available blog articles:\\n\\n\"\n",
        "    for i, article in enumerate(articles, 1):\n",
        "        source = article.get(\"_source\", article)\n",
        "        context += f\"Article {i}:\\n\"\n",
        "        context += f\"- Title: {source.get('title', 'N/A')}\\n\"\n",
        "        context += f\"- Author: {source.get('author', 'N/A')}\\n\"\n",
        "        context += f\"- Category: {source.get('category', 'N/A')}\\n\"\n",
        "        context += f\"- Date: {source.get('date', 'N/A')}\\n\"\n",
        "        context += f\"- Content: {source.get('content', 'N/A')}\\n\\n\"\n",
        "\n",
        "    # Simplified prompt that requests JSON in text\n",
        "    # Simplified prompt that requests JSON in text\n",
        "    system_prompt = \"\"\"You are an expert content curator that recommends blog articles.\n",
        "\n",
        "    Write recommendations in a conversational style starting with phrases like:\n",
        "    - \"If you're interested in [topic], this article...\"\n",
        "    - \"This post complements your search with...\"\n",
        "    - \"For those looking into [topic], this article provides...\"\n",
        "\n",
        "    Keep each recommendation concise (2-3 sentences max) and focused on VALUE to the reader.\n",
        "\n",
        "    Return ONLY a valid JSON object with this exact structure:\n",
        "    {\n",
        "        \"recommendations\": [\n",
        "            {\"article_number\": 1, \"recommendation\": \"your recommendation text\"},\n",
        "            {\"article_number\": 2, \"recommendation\": \"your recommendation text\"},\n",
        "            ... (continue for ALL articles)\n",
        "        ]\n",
        "    }\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Search query: \"{search_query}\"\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Generate a JSON object with one recommendation for EACH of the {len(articles)} articles above.\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    # LLM generation\n",
        "    print(f\"{'='*80}\")\n",
        "    print(\"ğŸ¤– Generating personalized recommendations...\\n\")\n",
        "\n",
        "    llm_start = time.time()\n",
        "    full_response = \"\"\n",
        "\n",
        "    for chunk in stream_chat_completion(messages):  # Sin response_format\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "        full_response += chunk\n",
        "\n",
        "    # Latency tracking\n",
        "    llm_end = time.time()\n",
        "    llm_latency = (llm_end - llm_start) * 1000\n",
        "    latencies[\"llm_generation\"] = llm_latency\n",
        "    latencies[\"total\"] = (llm_end - llm_start) * 1000\n",
        "\n",
        "    print(\n",
        "        f\"\\n\\n{'='*80}\\nğŸ“Š Latency Report:\\n  â€¢ LLM generation: {latencies['llm_generation']:.2f}ms\\n  â€¢ Total time: {latencies['total']:.2f}ms\\n{'='*80}\\n\"\n",
        "    )\n",
        "\n",
        "    return articles, full_response, latencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31bab62f",
      "metadata": {},
      "source": [
        "### Card visualization helper\n",
        "\n",
        "Function to display recommendations in a card-style format for better visual differentiation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "cb2f4981",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_recommendation_cards(articles, recommendations_text):\n",
        "    \"\"\"\n",
        "    Display article recommendations in a clean card format with title and personalized recommendation.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"ğŸ“‡ RECOMMENDED ARTICLES\".center(100))\n",
        "    print(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "    # Parse JSON recommendations - clean tags and extract JSON\n",
        "    recommendations_list = []\n",
        "    if recommendations_text:\n",
        "        try:\n",
        "            # Clean up <think> tags\n",
        "            cleaned_text = re.sub(\n",
        "                r\"<think>.*?</think>\", \"\", recommendations_text, flags=re.DOTALL\n",
        "            )\n",
        "            # Remove markdown code blocks ( ... ``` or ``` ... ```)\n",
        "            cleaned_text = re.sub(r\"```(?:json)?\", \"\", cleaned_text)\n",
        "            cleaned_text = cleaned_text.strip()\n",
        "\n",
        "            parsed = json.loads(cleaned_text)\n",
        "            recommendations_list = parsed.get(\"recommendations\", [])\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"âš ï¸  Could not parse recommendations as JSON: {e}\")\n",
        "            return\n",
        "\n",
        "    for i, article in enumerate(articles, 1):\n",
        "        source = article.get(\"_source\", article)\n",
        "\n",
        "        # Card border\n",
        "        print(\"â”Œ\" + \"â”€\" * 98 + \"â”\")\n",
        "\n",
        "        # Title\n",
        "        title = source.get(\"title\", \"N/A\")\n",
        "        title_lines = textwrap.wrap(f\"ğŸ“Œ {title}\", width=94)\n",
        "        for line in title_lines:\n",
        "            print(f\"â”‚  {line}\".ljust(99) + \"â”‚\")\n",
        "\n",
        "        # Card border\n",
        "        print(\"â”œ\" + \"â”€\" * 98 + \"â”¤\")\n",
        "\n",
        "        # Find recommendation for this article number\n",
        "        recommendation = None\n",
        "        for rec in recommendations_list:\n",
        "            if rec.get(\"article_number\") == i:\n",
        "                recommendation = rec.get(\"recommendation\")\n",
        "                break\n",
        "\n",
        "        # Only show if recommendation exists\n",
        "        if recommendation:\n",
        "            recommendation_lines = textwrap.wrap(recommendation, width=94)\n",
        "            for line in recommendation_lines:\n",
        "                print(f\"â”‚  {line}\".ljust(99) + \"â”‚\")\n",
        "\n",
        "        # Card bottom\n",
        "        print(\"â””\" + \"â”€\" * 98 + \"â”˜\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d9ea51",
      "metadata": {},
      "source": [
        "## Usage example: Article recommendation system\n",
        "\n",
        "Testing the recommendation system with Spanish articles to validate SmolLM3's multilingual capabilities and low latency performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "ce48fee9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ” Search Query: Security and vulnerabilities\n",
            "================================================================================\n",
            "\n",
            "âœ… Found 5 relevant articles\n",
            "\n",
            "================================================================================\n",
            "ğŸ¤– Generating personalized recommendations...\n",
            "\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "```json\n",
            "{\n",
            "    \"recommendations\": [\n",
            "        {\n",
            "            \"article_number\": 1,\n",
            "            \"recommendation\": \"If you're interested in security and vulnerabilities, this article from the Security Team highlights a critical vulnerability in their authentication system. They recommend updating to SDK version 3.2.1 immediately as affected tokens have been automatically revoked.\"\n",
            "        },\n",
            "        {\n",
            "            \"article_number\": 2,\n",
            "            \"recommendation\": \"This post complements your search with a detailed tutorial on migrating from version 1.x to 2.0, covering schema transformations and deprecated endpoints. It's essential for those planning the migration to do it in a staging environment first.\"\n",
            "        },\n",
            "        {\n",
            "            \"article_number\": 3,\n",
            "            \"recommendation\": \"For those looking into updating to the new architecture, this article provides valuable insights on database migrations and configuration changes that could break existing integrations. A complete impact assessment is recommended before proceeding.\"\n",
            "        },\n",
            "        {\n",
            "            \"article_number\": 4,\n",
            "            \"recommendation\": \"This critical update from the Development Team fixes a bug causing data loss in large-scale processing scenarios. If you've processed significant volumes of data recently, verify your data integrity and contact support if needed.\"\n",
            "        },\n",
            "        {\n",
            "            \"article_number\": 5,\n",
            "            \"recommendation\": \"Important changes to the notification system are detailed here, including push, email, SMS notifications, custom rules for alerts, and a notification history. A mobile client update is required for these new features.\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "================================================================================\n",
            "ğŸ“Š Latency Report:\n",
            "  â€¢ LLM generation: 10462.25ms\n",
            "  â€¢ Total time: 10462.25ms\n",
            "================================================================================\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "                                       ğŸ“‡ RECOMMENDED ARTICLES                                       \n",
            "====================================================================================================\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Security warning: Authentication system vulnerability                                         â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  If you're interested in security and vulnerabilities, this article from the Security Team       â”‚\n",
            "â”‚  highlights a critical vulnerability in their authentication system. They recommend updating to  â”‚\n",
            "â”‚  SDK version 3.2.1 immediately as affected tokens have been automatically revoked.               â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Known risks when migrating from version 1.x to 2.0                                            â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  This post complements your search with a detailed tutorial on migrating from version 1.x to     â”‚\n",
            "â”‚  2.0, covering schema transformations and deprecated endpoints. It's essential for those         â”‚\n",
            "â”‚  planning the migration to do it in a staging environment first.                                 â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Migration risk: Considerations for updating to the new architecture                           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  For those looking into updating to the new architecture, this article provides valuable         â”‚\n",
            "â”‚  insights on database migrations and configuration changes that could break existing             â”‚\n",
            "â”‚  integrations. A complete impact assessment is recommended before proceeding.                    â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Critical update: Bug fix in data processing                                                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  This critical update from the Development Team fixes a bug causing data loss in large-scale     â”‚\n",
            "â”‚  processing scenarios. If you've processed significant volumes of data recently, verify your     â”‚\n",
            "â”‚  data integrity and contact support if needed.                                                   â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Important changes to the notification system                                                  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  Important changes to the notification system are detailed here, including push, email, SMS      â”‚\n",
            "â”‚  notifications, custom rules for alerts, and a notification history. A mobile client update is   â”‚\n",
            "â”‚  required for these new features.                                                                â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "search_query = \"Security and vulnerabilities\"\n",
        "\n",
        "articles, recommendations, latencies = recommend_articles(search_query)\n",
        "\n",
        "# Display visual cards\n",
        "display_recommendation_cards(articles, recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44ce435",
      "metadata": {},
      "source": [
        "### Streaming function for real-time responses\n",
        "\n",
        "Send messages to the Elasticsearch inference endpoint with streaming support, processing server-sent events to extract model responses in real-time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fbeb64b",
      "metadata": {},
      "source": [
        "`ask_question_streaming` function to put together the semantic search and the real time chat_completions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8311936",
      "metadata": {},
      "source": [
        "## Use example\n",
        "\n",
        "We'll ask about the articles that mention risks, vulnerabilities, or warnings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C2WKnj8UZa7g",
      "metadata": {
        "id": "C2WKnj8UZa7g"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "Delete the index and inference endpoints to prevent consuming resources after completing the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "TgqFHEhPZfAd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgqFHEhPZfAd",
        "outputId": "675e161b-f7e3-43f8-d3cc-f9e7fa72f6aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index blog-posts deleted\n"
          ]
        }
      ],
      "source": [
        "# Cleanup - Delete Index\n",
        "es_client.indices.delete(index=INDEX_NAME)\n",
        "print(f\"Index {INDEX_NAME} deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcc72db",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ObjectApiResponse({'acknowledged': True})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cleanup - Delete Inference Endpoint\n",
        "es_client.inference.delete(inference_id=INFERENCE_ENDPOINT_ID)\n",
        "print(f\"Inference endpoint {INFERENCE_ENDPOINT_ID} deleted\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "C3EFzwK9YbaP",
   "metadata": {
    "id": "C3EFzwK9YbaP"
   },
   "source": [
    "# Using Elasticsearch Inference API along Hugging Face models\n",
    "\n",
    "This notebook demonstrates how to use the Elasticsearch Inference API along with Hugging Face models to build a question and answer system. This notebook is based on the [Using Elasticsearch Inference API along Hugging Face models](https://www.elastic.co/search-labs/blog/elasticsearch-inference-api-and-hugging-face)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bb30f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74bb30f2",
    "outputId": "185bf00e-67fb-4504-e56c-30f8c8872f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests elasticsearch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TgEea48gYnp5",
   "metadata": {
    "id": "TgEea48gYnp5"
   },
   "source": [
    "## Installing dependencies and importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4668548",
   "metadata": {
    "id": "b4668548"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a30c9",
   "metadata": {},
   "source": [
    "## Setting up environment variables\n",
    "\n",
    "Configure API keys and URLs for Elasticsearch and Hugging Face, along with the index name and inference endpoint identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01aab023",
   "metadata": {
    "id": "01aab023"
   },
   "outputs": [],
   "source": [
    "ELASTICSEARCH_API_KEY = os.getenv(\"ELASTICSEARCH_API_KEY\")\n",
    "ELASTICSEARCH_URL = os.getenv(\"ELASTICSEARCH_URL\")\n",
    "HUGGING_FACE_API_KEY = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "HUGGING_FACE_INFERENCE_ENDPOINT_URL = os.getenv(\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\")\n",
    "\n",
    "\n",
    "INDEX_NAME = \"blog-posts\"\n",
    "INFERENCE_ENDPOINT_ID = \"hugging-face-mistral-7b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjbXck_gc9lY",
   "metadata": {
    "id": "tjbXck_gc9lY"
   },
   "source": [
    "## Elasticsearch Python client\n",
    "\n",
    "Initialize the Elasticsearch client using the configured URL and API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf922ab",
   "metadata": {
    "id": "8cf922ab"
   },
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(ELASTICSEARCH_URL, api_key=ELASTICSEARCH_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832186f0",
   "metadata": {},
   "source": [
    "## Hugging Face completions inference endpoint setup\n",
    "\n",
    "Create an Elasticsearch inference endpoint that connects to the Hugging Face model for generating responses based on blog articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40eba1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion inference endpoint created successfully: hugging-face-mistral-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resp = es_client.inference.put(\n",
    "        task_type=\"chat_completion\",\n",
    "        inference_id=INFERENCE_ENDPOINT_ID,\n",
    "        body={\n",
    "            \"service\": \"hugging_face\",\n",
    "            \"service_settings\": {\n",
    "                \"api_key\": HUGGING_FACE_API_KEY,\n",
    "                \"url\": HUGGING_FACE_INFERENCE_ENDPOINT_URL,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Chat completion inference endpoint created successfully:\",\n",
    "        resp[\"inference_id\"],\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error creating chat completion inference endpoint:\", {e})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21438e7e",
   "metadata": {},
   "source": [
    "### Creating index mapping\n",
    "\n",
    "Define field types and properties for the blog articles index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fbe4d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index blog-posts created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"copy_to\": \"semantic_field\",\n",
    "                    \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
    "                },\n",
    "                \"author\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "                \"category\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "                \"content\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
    "                \"date\": {\"type\": \"date\"},\n",
    "                \"semantic_field\": {\"type\": \"semantic_text\"},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    es_client.indices.create(index=INDEX_NAME, body=mapping)\n",
    "    print(f\"Index {INDEX_NAME} created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dd56355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 documents indexed successfully\n"
     ]
    }
   ],
   "source": [
    "def build_data(json_file, index_name):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for doc in data:\n",
    "        action = {\"_index\": index_name, \"_source\": doc}\n",
    "        yield action\n",
    "\n",
    "\n",
    "try:\n",
    "    success, failed = helpers.bulk(\n",
    "        es_client,\n",
    "        build_data(\"dataset.json\", INDEX_NAME),\n",
    "    )\n",
    "    print(f\"{success} documents indexed successfully\")\n",
    "\n",
    "    if failed:\n",
    "        print(f\"Errors: {failed}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce36a4",
   "metadata": {},
   "source": [
    "## Semantic search function\n",
    "\n",
    "Function to search for relevant articles using Elasticsearch semantic search capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99600e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_articles(query_text, index_name=INDEX_NAME, size=5):\n",
    "    try:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"semantic\": {\n",
    "                    \"field\": \"semantic_field\",\n",
    "                    \"query\": query_text,\n",
    "                }\n",
    "            },\n",
    "            \"size\": size,\n",
    "        }\n",
    "\n",
    "        response = es_client.search(index=index_name, body=query)\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(f\"Semantic search error: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ce435",
   "metadata": {},
   "source": [
    "### Streaming function for real-time responses\n",
    "\n",
    "Send messages to the Elasticsearch inference endpoint with streaming support, processing server-sent events to extract model responses in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "530b687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Streaming function defined!\n"
     ]
    }
   ],
   "source": [
    "def stream_chat_completion(messages: list, inference_id: str = INFERENCE_ENDPOINT_ID):\n",
    "\n",
    "    url = f\"{ELASTICSEARCH_URL}/_inference/chat_completion/{inference_id}/_stream\"\n",
    "    payload = {\"messages\": messages}\n",
    "    headers = {\n",
    "        \"Authorization\": f\"ApiKey {ELASTICSEARCH_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if line:\n",
    "                line = line.strip()\n",
    "\n",
    "                # Handle Server-Sent Events format\n",
    "                # Skip event lines like \"event: message\"\n",
    "                if line.startswith(\"event:\"):\n",
    "                    continue\n",
    "\n",
    "                # Process data lines\n",
    "                if line.startswith(\"data: \"):\n",
    "                    data_content = line[6:]  # Remove \"data: \" prefix\n",
    "\n",
    "                    # Skip empty data or special markers\n",
    "                    if not data_content.strip() or data_content.strip() == \"[DONE]\":\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        chunk_data = json.loads(data_content)\n",
    "\n",
    "                        # Extract the content from the response structure\n",
    "                        if \"choices\" in chunk_data and len(chunk_data[\"choices\"]) > 0:\n",
    "                            choice = chunk_data[\"choices\"][0]\n",
    "                            if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
    "                                content = choice[\"delta\"][\"content\"]\n",
    "                                if content:  # Only yield non-empty content\n",
    "                                    yield content\n",
    "\n",
    "                    except json.JSONDecodeError as json_err:\n",
    "                        # If JSON parsing fails, log the error but continue\n",
    "                        print(f\"\\nJSON decode error: {json_err}\")\n",
    "                        print(f\"Problematic data: {data_content}\")\n",
    "                        continue\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        yield f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"✅ Streaming function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbeb64b",
   "metadata": {},
   "source": [
    "`ask_question_streaming` function to put together the semantic search and the real time chat_completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_streaming(user_question, index_name=INDEX_NAME, max_articles=5):\n",
    "\n",
    "    # Search for relevant articles\n",
    "    articles = search_articles(user_question, index_name, size=max_articles)\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No relevant articles found for your question.\")\n",
    "        return\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Semantic search results: {json.dumps(articles, indent=2)}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Build context with found articles\n",
    "    context = \"Relevant articles found:\\n\\n\"\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        source = article.get(\"_source\", article)\n",
    "        context += f\"Article {i}:\\n\"\n",
    "        context += f\"Title: {source.get('title', 'N/A')}\\n\"\n",
    "        context += f\"Author: {source.get('author', 'N/A')}\\n\"\n",
    "        context += f\"Category: {source.get('category', 'N/A')}\\n\"\n",
    "        context += f\"Date: {source.get('date', 'N/A')}\\n\"\n",
    "        context += f\"Content: {source.get('content', 'N/A')}\\n\\n\"\n",
    "\n",
    "    # Build the prompt for the model\n",
    "    system_prompt = \"\"\"You are an expert assistant that helps answer questions about blog articles.\n",
    "    Based on the provided articles, answer the user's question clearly and accurately.\n",
    "    If the information is not available in the articles, clearly indicate so.\n",
    "    Cite relevant articles when appropriate.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"User question: {user_question}\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Please answer the user's question based on the information from the provided articles.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    print(f\"Question: {user_question}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Response (streaming):\\n\")\n",
    "\n",
    "    # Stream the response\n",
    "    for chunk in stream_chat_completion(messages):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8311936",
   "metadata": {},
   "source": [
    "## Use example\n",
    "\n",
    "We'll ask about the articles that mention risks, vulnerabilities, or warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d6659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Semantic search results: [\n",
      "  {\n",
      "    \"_index\": \"blog-posts\",\n",
      "    \"_id\": \"Knww6poBDe8qGaqI7gFQ\",\n",
      "    \"_score\": 9.997918,\n",
      "    \"_source\": {\n",
      "      \"id\": \"2\",\n",
      "      \"title\": \"Security warning: Authentication system vulnerability\",\n",
      "      \"author\": \"Security Team\",\n",
      "      \"date\": \"2025-11-02\",\n",
      "      \"category\": \"security\",\n",
      "      \"content\": \"We have identified a critical vulnerability in the authentication system that could allow unauthorized access. The vulnerability affects users using JWT tokens issued before October 15th. We recommend updating immediately to SDK version 3.2.1. All affected tokens have been automatically revoked. Please regenerate your access credentials.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"_index\": \"blog-posts\",\n",
      "    \"_id\": \"LXww6poBDe8qGaqI7gFQ\",\n",
      "    \"_score\": 8.438947,\n",
      "    \"_source\": {\n",
      "      \"id\": \"5\",\n",
      "      \"title\": \"Known risks when migrating from version 1.x to 2.0\",\n",
      "      \"author\": \"Laura Perez\",\n",
      "      \"date\": \"2025-11-05\",\n",
      "      \"category\": \"tutorial\",\n",
      "      \"content\": \"If you're planning to migrate from version 1.x to 2.0, there are several important risks to consider. The most significant change is the new data structure that requires schema transformation. Some endpoints have been deprecated and others have changes in required parameters. We recommend doing the migration in a staging environment first and validating all integrations. We have created a detailed migration guide.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"_index\": \"blog-posts\",\n",
      "    \"_id\": \"Nnww6poBDe8qGaqI7gFQ\",\n",
      "    \"_score\": 7.4116235,\n",
      "    \"_source\": {\n",
      "      \"id\": \"14\",\n",
      "      \"title\": \"Migration risk: Considerations for updating to the new architecture\",\n",
      "      \"author\": \"Jessica Martinez\",\n",
      "      \"date\": \"2025-11-14\",\n",
      "      \"category\": \"tutorial\",\n",
      "      \"content\": \"If you're considering updating to the new architecture, there are several risks to consider. The most important change is the database migration that requires planned downtime. There are also changes in the configuration structure that may break existing integrations. We recommend doing a complete impact assessment before proceeding. We have created a migration checklist available in our repository.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"_index\": \"blog-posts\",\n",
      "    \"_id\": \"Mnww6poBDe8qGaqI7gFQ\",\n",
      "    \"_score\": 6.549363,\n",
      "    \"_source\": {\n",
      "      \"id\": \"10\",\n",
      "      \"title\": \"Comparative analysis: Technical vs. introductory articles\",\n",
      "      \"author\": \"Pedro Alvarez\",\n",
      "      \"date\": \"2025-11-10\",\n",
      "      \"category\": \"analysis\",\n",
      "      \"content\": \"We have analyzed the most popular articles in our community and found interesting patterns. Deep technical articles (with code and advanced examples) have higher long-term engagement, while introductory articles have more initial traffic. Technical articles are often more shared in developer communities, while introductory articles are better for SEO and new user acquisition.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"_index\": \"blog-posts\",\n",
      "    \"_id\": \"NXww6poBDe8qGaqI7gFQ\",\n",
      "    \"_score\": 6.241748,\n",
      "    \"_source\": {\n",
      "      \"id\": \"13\",\n",
      "      \"title\": \"Summary of articles about data visualizations\",\n",
      "      \"author\": \"Alex Turner\",\n",
      "      \"date\": \"2025-11-13\",\n",
      "      \"category\": \"summary\",\n",
      "      \"content\": \"This article summarizes all available resources about visualizations on our platform. It covers everything from basic tutorials to advanced customization guides. It includes references to articles about chart types, design best practices, performance optimization, and specific use cases. Ideal for users who want to learn about visualizations in a structured way.\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "================================================================================\n",
      "Question: Are there any articles that mention risks, vulnerabilities, or warnings?\n",
      "\n",
      "================================================================================\n",
      "Response (streaming):\n",
      "\n",
      " Yes, there are several articles that mention risks, vulnerabilities, or warnings in the provided set. Here are some examples:\n",
      "\n",
      "1. Article 1 titled \"Security warning: Authentication system vulnerability\" discusses a critical vulnerability in the authentication system that could allow unauthorized access. The article provides details about the affected users and recommends immediate updates to address this issue.\n",
      "\n",
      "2. Article 2 titled \"Known risks when migrating from version 1.x to 2.0\" mentions several risks associated with migrating from version 1.x to 2.0, including changes in data structure, deprecated endpoints, and required parameter changes that may affect integrations.\n",
      "\n",
      "3. Article 3 titled \"Migration risk: Considerations for updating to the new architecture\" also discusses risks related to updating to a new architecture, particularly focusing on the database migration that requires planned downtime and potential breaks in existing integrations.\n",
      "\n",
      "These articles provide useful information about various risks and vulnerabilities associated with different aspects of the system or its updates.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "ask_question_streaming(\n",
    "    \"Are there any articles that mention risks, vulnerabilities, or warnings?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C2WKnj8UZa7g",
   "metadata": {
    "id": "C2WKnj8UZa7g"
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the index and inference endpoints to prevent consuming resources after completing the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "TgqFHEhPZfAd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgqFHEhPZfAd",
    "outputId": "675e161b-f7e3-43f8-d3cc-f9e7fa72f6aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index blog-posts deleted\n"
     ]
    }
   ],
   "source": [
    "# Cleanup - Delete Index\n",
    "es_client.indices.delete(index=INDEX_NAME)\n",
    "print(f\"Index {INDEX_NAME} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc72db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup - Delete Inference Endpoint\n",
    "es_client.inference.delete(inference_id=INFERENCE_ENDPOINT_ID)\n",
    "print(f\"Inference endpoint {INFERENCE_ENDPOINT_ID} deleted\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "C3EFzwK9YbaP",
      "metadata": {
        "id": "C3EFzwK9YbaP"
      },
      "source": [
        "# Elasticsearch Inference API & Hugging Face\n",
        "\n",
        "This notebook demonstrates how to use Hugging Face completions along with the Elasticsearch Inference API. This notebook is based on the article [Inference API & and Hugging Face](https://www.elastic.co/search-labs/blog/inference-api-and-hugging-face)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "74bb30f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74bb30f2",
        "outputId": "185bf00e-67fb-4504-e56c-30f8c8872f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install requests elasticsearch -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TgEea48gYnp5",
      "metadata": {
        "id": "TgEea48gYnp5"
      },
      "source": [
        "## Installing dependencies and importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b4668548",
      "metadata": {
        "id": "b4668548"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7a30c9",
      "metadata": {},
      "source": [
        "## Setting up environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01aab023",
      "metadata": {
        "id": "01aab023"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\"] = getpass(\n",
        "    \"Enter your Hugging Face Inference Endpoint URL: \"\n",
        ")\n",
        "os.environ[\"ELASTICSEARCH_API_KEY\"] = getpass(\"Enter your Elasticsearch API key: \")\n",
        "os.environ[\"ELASTICSEARCH_URL\"] = getpass(\"Enter your Elasticsearch URL: \")\n",
        "os.environ[\"HUGGING_FACE_API_KEY\"] = getpass(\"Enter your Hugging Face API key: \")\n",
        "\n",
        "\n",
        "INDEX_NAME = \"company-blog-posts\"\n",
        "INFERENCE_ENDPOINT_ID = \"hugging-face-gpt-oss-safeguard\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tjbXck_gc9lY",
      "metadata": {
        "id": "tjbXck_gc9lY"
      },
      "source": [
        "## Elasticsearch Python client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8cf922ab",
      "metadata": {
        "id": "8cf922ab"
      },
      "outputs": [],
      "source": [
        "es_client = Elasticsearch(\n",
        "    os.environ[\"ELASTICSEARCH_URL\"], api_key=os.environ[\"ELASTICSEARCH_API_KEY\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832186f0",
      "metadata": {},
      "source": [
        "## Hugging Face completions inference endpoint setup\n",
        "\n",
        "Let’s create an Elasticsearch inference endpoint that uses the Hugging Face model. This endpoint will be used for chat completions, and this service will help us generate responses based on the dataset. To create this endpoint we’re using the [inference API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40eba1a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'inference_id': 'hugging-face-gpt-oss-safeguard', 'task_type': 'chat_completion', 'service': 'hugging_face', 'service_settings': {'url': 'https://ibuf1kbe7xcak382.us-east-1.aws.endpoints.huggingface.cloud/v1/chat/completions', 'rate_limit': {'requests_per_minute': 3000}}}\n",
            "Chat completion inference endpoint created successfully: hugging-face-gpt-oss-safeguard\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    resp = es_client.inference.put(\n",
        "        task_type=\"chat_completion\",\n",
        "        inference_id=INFERENCE_ENDPOINT_ID,\n",
        "        body={\n",
        "            \"service\": \"hugging_face\",\n",
        "            \"service_settings\": {\n",
        "                \"api_key\": os.environ[\"HUGGING_FACE_API_KEY\"],\n",
        "                \"url\": os.environ[\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\"],\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Chat completion inference endpoint created successfully:\", resp[\"inference_id\"]\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Error creating chat completion inference endpoint:\", {e})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iRX6_LuXdBy1",
      "metadata": {
        "id": "iRX6_LuXdBy1"
      },
      "source": [
        "## Index setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead65986",
      "metadata": {},
      "source": [
        "### Creating mappings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3fc74243",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fc74243",
        "outputId": "d0f171e6-fcb4-48be-b9e6-11682410cf4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index company-blog-posts created successfully\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    mapping = {\n",
        "        \"mappings\": {\n",
        "            \"properties\": {\n",
        "                \"id\": {\"type\": \"keyword\"},\n",
        "                \"title\": {\n",
        "                    \"type\": \"text\",\n",
        "                    \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
        "                    \"copy_to\": \"semantic_field\",\n",
        "                },\n",
        "                \"author\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
        "                \"category\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
        "                \"content\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
        "                \"date\": {\"type\": \"date\"},\n",
        "                \"semantic_field\": {\"type\": \"semantic_text\"},\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    es_client.indices.create(index=INDEX_NAME, body=mapping)\n",
        "    print(f\"Index {INDEX_NAME} created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating index: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sBDsH4VCZNBX",
      "metadata": {
        "id": "sBDsH4VCZNBX"
      },
      "source": [
        "### Ingesting data to Elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "vuPAJRXLY_-J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuPAJRXLY_-J",
        "outputId": "29b17505-0150-4ea3-b4d0-cf5fd192d1b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 documents indexed successfully\n"
          ]
        }
      ],
      "source": [
        "def build_data(json_file, index_name):\n",
        "    with open(json_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for doc in data:\n",
        "        yield {\"_index\": index_name, \"_source\": doc}\n",
        "\n",
        "\n",
        "try:\n",
        "    success, errors = helpers.bulk(es_client, build_data(\"dataset.json\", INDEX_NAME))\n",
        "    print(f\"{success} documents indexed successfully\")\n",
        "\n",
        "    if errors:\n",
        "        print(\"Errors during indexing:\", errors)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gZD2xqc-ZW88",
      "metadata": {
        "id": "gZD2xqc-ZW88"
      },
      "source": [
        "## Function to execute semantic search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "R54-_-0IZITC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R54-_-0IZITC",
        "outputId": "a243c9d5-1687-449e-f12a-7d82a0f592b3"
      },
      "outputs": [],
      "source": [
        "def semantic_search(user_question: str, size: int = 5):\n",
        "    try:\n",
        "        response = es_client.search(\n",
        "            index=INDEX_NAME,\n",
        "            body={\n",
        "                \"query\": {\"match\": {\"semantic_field\": user_question}},\n",
        "                \"size\": size,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"hits\": response[\"hits\"][\"hits\"],\n",
        "            \"total_hits\": len(response[\"hits\"][\"hits\"]),\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching index: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a82dc6df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total hits: 1\n",
            "{\n",
            "  \"id\": \"8\",\n",
            "  \"title\": \"Security update: encrypted data pipelines\",\n",
            "  \"author\": \"Daniel Vega\",\n",
            "  \"date\": \"2025-11-08\",\n",
            "  \"category\": \"security\",\n",
            "  \"content\": \"We have enhanced our data pipelines with full encryption in transit and at rest. This aligns with our company\\u2019s commitment to protecting customer data.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "results = semantic_search(\n",
        "    user_question=\"Search for updates related to data encryption.\", size=1\n",
        ")\n",
        "\n",
        "print(f\"Total hits: {results['total_hits']}\")\n",
        "for hit in results[\"hits\"]:\n",
        "    print(f\"{json.dumps(hit['_source'], indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44ce435",
      "metadata": {},
      "source": [
        "## Generating completions function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "530b687e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_chat_completion(messages: list, inference_id: str = INFERENCE_ENDPOINT_ID):\n",
        "    url = f\"{os.environ['ELASTICSEARCH_URL']}/_inference/chat_completion/{inference_id}/_stream\"\n",
        "\n",
        "    payload = {\"messages\": messages}\n",
        "\n",
        "    ELASTICSEARCH_HEADERS = {\n",
        "        \"Authorization\": f\"ApiKey {os.environ['ELASTICSEARCH_API_KEY']}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            url, json=payload, headers=ELASTICSEARCH_HEADERS, stream=True\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "\n",
        "        for line in response.iter_lines(decode_unicode=True):\n",
        "            if line:\n",
        "                line = line.strip()\n",
        "\n",
        "                # Skip event lines like \"event: message\"\n",
        "                if line.startswith(\"event:\"):\n",
        "                    continue\n",
        "\n",
        "                # Process data lines\n",
        "                if line.startswith(\"data: \"):\n",
        "                    data_content = line[6:]  # Remove \"data: \" prefix\n",
        "\n",
        "                    # Skip empty data or special markers\n",
        "                    if not data_content.strip() or data_content.strip() == \"[DONE]\":\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        chunk_data = json.loads(data_content)\n",
        "\n",
        "                        # Extract the content from the Mistral response structure\n",
        "                        if \"choices\" in chunk_data and len(chunk_data[\"choices\"]) > 0:\n",
        "                            choice = chunk_data[\"choices\"][0]\n",
        "                            if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
        "                                content = choice[\"delta\"][\"content\"]\n",
        "                                if content:  # Only yield non-empty content\n",
        "                                    yield content\n",
        "\n",
        "                    except json.JSONDecodeError as json_err:\n",
        "                        # If JSON parsing fails, log the error but continue\n",
        "                        print(f\"\\nJSON decode error: {json_err}\")\n",
        "                        print(f\"Problematic data: {data_content}\")\n",
        "                        continue\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        yield f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9af051c",
      "metadata": {},
      "source": [
        "## RAG Chat with Streaming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e720494",
      "metadata": {},
      "outputs": [],
      "source": [
        "def moderation_chat(\n",
        "    user_query: str,\n",
        "    inference_id: str = INFERENCE_ENDPOINT_ID,\n",
        "    ruleset_path: str = \"policies.txt\",\n",
        "):\n",
        "    search_results = semantic_search(user_query)\n",
        "    context_docs = search_results[\"hits\"]\n",
        "\n",
        "    try:\n",
        "        with open(ruleset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            ruleset = f.read()\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\n",
        "            \"ruleset.txt not found. Please provide the company policies file.\"\n",
        "        )\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "        You are a Safeguard moderation assistant.\n",
        "        Evaluate the provided article according to the company content policies.\n",
        "\n",
        "        The evaluation must follow these instructions:\n",
        "        - Only use the provided ruleset to judge compliance.\n",
        "        - Focus on tone, confidentiality, and adherence to internal communication standards.\n",
        "\n",
        "        Company ruleset:\n",
        "        {ruleset}\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, doc in enumerate(context_docs, 1):\n",
        "        title = doc[\"_source\"].get(\"title\", f\"Article {i}\")\n",
        "        content = doc[\"_source\"].get(\"content\", \"\")\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "            Evaluate this article:\n",
        "            Title: {title}\n",
        "            Content body:\n",
        "            {content}\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "\n",
        "        print(\"\\n------------------------------------------------\")\n",
        "        print(\n",
        "            f\"Evaluating article {i}/{len(context_docs)}\\n TITLE: {title} \\n CONTENT: {content}...\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            full_response = \"\"\n",
        "            for chunk in stream_chat_completion(messages, inference_id=inference_id):\n",
        "                print(chunk, end=\"\", flush=True)\n",
        "                full_response += chunk\n",
        "\n",
        "            results.append({\"title\": title, \"response\": full_response.strip()})\n",
        "        except Exception as e:\n",
        "            print(f\"\\n⚠️ Error evaluating {title}: {e}\")\n",
        "            results.append({\"title\": title, \"response\": None, \"error\": str(e)})\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e04a7795",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'_index': 'company-blog-posts', '_id': 'Xp1ngpoB2AKc8jP6brQU', '_score': 32.036026, '_source': {'id': '6', 'title': 'Debugging authentication issues', 'author': 'Tomás Hernández', 'date': '2025-11-06', 'category': 'engineering', 'content': 'When debugging authentication, never share production tokens or client credentials. Always use our test environment and anonymized data for public examples.'}}, {'_index': 'company-blog-posts', '_id': 'X51ngpoB2AKc8jP6brQU', '_score': 5.9653068, '_source': {'id': '7', 'title': 'Meet our competitors head-on!', 'author': 'Alice Walker', 'date': '2025-11-07', 'category': 'marketing', 'content': 'Unlike BrandX, our software is actually stable and bug-free. This article compares us directly to competitors and mocks their quality issues.'}}, {'_index': 'company-blog-posts', '_id': 'YJ1ngpoB2AKc8jP6brQU', '_score': 4.9244947, '_source': {'id': '8', 'title': 'Security update: encrypted data pipelines', 'author': 'Daniel Vega', 'date': '2025-11-08', 'category': 'security', 'content': 'We have enhanced our data pipelines with full encryption in transit and at rest. This aligns with our company’s commitment to protecting customer data.'}}, {'_index': 'company-blog-posts', '_id': 'Yp1ngpoB2AKc8jP6brQU', '_score': 4.920317, '_source': {'id': '10', 'title': 'Client feedback and internal complaints', 'author': 'Pedro Alvarez', 'date': '2025-11-10', 'category': 'support', 'content': 'Some clients are hard to deal with and frankly don’t know how to use our software. In this post I’ll share examples from their support tickets.'}}, {'_index': 'company-blog-posts', '_id': 'YZ1ngpoB2AKc8jP6brQU', '_score': 4.1131787, '_source': {'id': '9', 'title': 'Casual Friday: fun at the office!', 'author': 'Lucía Martínez', 'date': '2025-11-09', 'category': 'culture', 'content': 'Sharing a few photos of our last Casual Friday. Everyone had a great time! This blog post respects internal media policy, with consent from all participants.'}}]\n",
            "\n",
            "------------------------------------------------\n",
            "Evaluating article 1/5\n",
            " TITLE: Debugging authentication issues \n",
            " CONTENT: When debugging authentication, never share production tokens or client credentials. Always use our test environment and anonymized data for public examples....\n",
            "\n",
            " The provided article does not appear to violate any of the provided rules. It provides a factual and professional tone, does not disclose any confidential information, and does not make any comparative claims or include personal data without explicit consent. The article also encourages safe practices, specifically stressing the importance of not sharing production tokens or client credentials. Overall, the article appears to be safe content according to the company's content policies.\n",
            "------------------------------------------------\n",
            "Evaluating article 2/5\n",
            " TITLE: Meet our competitors head-on! \n",
            " CONTENT: Unlike BrandX, our software is actually stable and bug-free. This article compares us directly to competitors and mocks their quality issues....\n",
            "\n",
            " Violation: The article falls under the category of Unprofessional tone due to the mocking language used towards a competitor, named BrandX, in the content body. This goes against the company's content policies by using language that is unrespectful and not aligned with corporate communication standards. Additionally, the content comparison could be considered a Comparative claim. Therefore, the article violates rules 3 and 5 of the provided company ruleset.\n",
            "------------------------------------------------\n",
            "Evaluating article 3/5\n",
            " TITLE: Security update: encrypted data pipelines \n",
            " CONTENT: We have enhanced our data pipelines with full encryption in transit and at rest. This aligns with our company’s commitment to protecting customer data....\n",
            "\n",
            " The given article is safe according to the provided company content policies. It does not violate any of the criteria mentioned in the ruleset. The tone used is professional and factual, and it does not disclose any confidential information or internal project details, nor does it make comparative claims, share personal data, or encourage unsafe practices. The article is focused on a public feature (data encryption) and its implementation (enhancing the data pipelines). Therefore, it aligns with the safe content criteria of the ruleset.\n",
            "------------------------------------------------\n",
            "Evaluating article 4/5\n",
            " TITLE: Client feedback and internal complaints \n",
            " CONTENT: Some clients are hard to deal with and frankly don’t know how to use our software. In this post I’ll share examples from their support tickets....\n",
            "\n",
            " VIOLATION: This article violates rule 4 (Inclusion of personal or client information without explicit consent). The contents of support tickets likely contain personal data or identifiable client information, which should not be shared without explicit consent from the parties involved. Additionally, the tone used in the article may be considered unprofessional and disrespectful, which goes against rule 3 (Use of unprofessional, offensive, or mocking language toward clients).\n",
            "------------------------------------------------\n",
            "Evaluating article 5/5\n",
            " TITLE: Casual Friday: fun at the office! \n",
            " CONTENT: Sharing a few photos of our last Casual Friday. Everyone had a great time! This blog post respects internal media policy, with consent from all participants....\n",
            "\n",
            " The provided article titled \"Casual Friday: fun at the office!\" does not seem to violate any of the company's content policies. It follows the criteria for safe content by focusing on a public event (Casual Friday), showing respect for internal media policy, and using a respectful and factual tone. The article does not disclose confidential information, internal project codenames, internal pricing, or financial details, nor does it include personal data without consent or make comparative claims in an unprofessional or offensive manner. Additionally, the article does not encourage unsafe practices. Therefore, this article is compliant with the company's content policies."
          ]
        }
      ],
      "source": [
        "response = moderation_chat(\n",
        "    user_query=\"Find posts explaining debugging or authentication issues\"\n",
        ")\n",
        "# Find posts explaining debugging or authentication issues.\n",
        "\n",
        "# “Find articles about analytics dashboards or visualization tools.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cfa3d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = moderation_chat(\n",
        "    \"Find posts explaining debugging or authentication issues\",\n",
        "    inference_id=\"gpt-oss-20b-endpoint\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C2WKnj8UZa7g",
      "metadata": {
        "id": "C2WKnj8UZa7g"
      },
      "source": [
        "## Deleting\n",
        "\n",
        "Delete the resources used to prevent them from consuming resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "TgqFHEhPZfAd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgqFHEhPZfAd",
        "outputId": "675e161b-f7e3-43f8-d3cc-f9e7fa72f6aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ObjectApiResponse({'acknowledged': True})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cleanup - Delete Index\n",
        "es_client.indices.delete(index=INDEX_NAME)\n",
        "# TODO: delete inference"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
